# Summary of Project:
- The music App Streaming startup Sparkify has grown their user base and song database. Now they want to move their processes and data on cloud.

# The information regarding datasides residing in S3:
1. Log Data
•	These are the log files generated by event simulator based on song in song dataset
•	These files are partitioned by year and month
2. Song Data:
•	Contains information about the songs and artist of that song.
•	These files are in JSON format
•	The files are partitioned by first three letters of each song’s track ID

# Creation of IAM Role and Redshift Cluster in Amazon
1. Login to AWS Console
2. From services open Idetity and Access Management Console (IAM).
3. Add user using programatic access and give administrator access to this user using attach policy and then select AdministratorAccess.
4. Download the CSV containing credentials of this IAM role.
5. Add key and Secret in dwh.cfg and save the file.
6. Use Infrasture_As_Code.ipynb file to add redshift AmazonS3ReadOnlyAccess access, create and delete redshift clusture.

# The steps to perform this analysis:
1. EXTRACT - Extract data from JSON Logs and JSON Metadata from S3 and load it in staging table of Redshift.
2. TRANSFORM - Filtering records by "NextSong" Action. Select only required columns from staging table and load into normal tables.
3. LOAD - Loading data into target tables i.e. Songplays, Songs, Users, Artists, Time.

# Information regarding files in repository:
1.	Create_tables.py - This file contains queries to Drop tables if exists and Create tables.
2.	sql_queries.py - This file stores all queries in form of variables and these variables are then getting called in Create_tables.py for execution.
3.	etl.py - This file contains the queries to load all the provided files (log_data and song_data) into staging table and then from staging table inserting records into dimension tables users, songs, artist, time and fact table songplays.
4.	test.ipynb - This file is to test the data in tables.
5.	dwh.cfg – This is configuarion file to set IAM roles key and location of source files in S3 bucket.
6.

# How to run python scripts?
1. Go to Terminal and then type command python create_tables.py and press enter. All the staging, fact and dimension tables are created with this file.
2. Then type python etl.py and then press enter. After successful execution of this command, all files from log data and song data are loaded into fact and dimension tables.
