{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions  import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types  import TimestampType \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']= config.get(\"AWS\",\"AWS_ACCESS_KEY_ID\")\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='config.get(\"AWS\",\"AWS_SECRET_ACCESS_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "song_data = os.path.join(input_data, \"song-data/A/B/A/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_song_data = spark.read.json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_song_data = spark.read.format(\"json\").load(\"s3a://udacity-dend/song_data/A/B/A\")\n",
    "\n",
    "print(df_song_data.count())\n",
    "\n",
    "df_song_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_song_data.createOrReplaceTempView(\"song_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_table = spark.sql(\"\"\"\n",
    "    SELECT distinct song_id, title, artist_id, year, duration\n",
    "    FROM song_data\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = \"s3a://ywaranass3bucket/DataLake-Project/\"\n",
    "songs_table.write.mode('overwrite').partitionBy(\"year\", \"artist_id\").parquet( os.path.join(output_data ,'songs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_table = spark.sql(\"\"\"\n",
    "    SELECT distinct artist_id,artist_name,artist_location,artist_latitude,artist_longitude\n",
    "    FROM song_data\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = 's3a://udacity-dend/log_data/2018/11/2018-11-07-events.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_data = spark.read.json(log_data)\n",
    "df_log_data_filtered = df_log_data[df_log_data.page==\"NextSong\"]\n",
    "df_log_data_filtered.createOrReplaceTempView(\"log_data_ftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table = spark.sql(\"\"\"\n",
    "    SELECT distinct userId,firstName,lastName,gender,level\n",
    "    FROM log_data_ftr \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_timestamp = udf(lambda x: datetime.fromtimestamp((x/1000.0)), TimestampType())\n",
    "df_log_data_filtered = df_log_data_filtered.withColumn(\"newts\", get_timestamp(df_log_data_filtered.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_datetime = udf(lambda x: datetime.fromtimestamp((x/1000.0)).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "df_log_data_filtered = df_log_data_filtered.withColumn(\"datetime\", get_datetime(df_log_data_filtered_timestamp.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_data_filtered_datetime.createOrReplaceTempView(\"time_data\")\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT ts as start_time,\n",
    "           hour(datetime) as hour,\n",
    "           dayofmonth(datetime) as day,\n",
    "           weekofyear(datetime) as week,\n",
    "           month(datetime) as month,\n",
    "           year(datetime) as year,\n",
    "           weekday(datetime) as weekday\n",
    "      FROM time_data \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table.write.mode('overwrite').partitionBy(\"year\", \"month\").parquet( \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "         ROW_NUMBER() OVER (ORDER BY start_time,user_id,level,song_id,artist_id,session_id,location,user_agent) as songplay_id\n",
    "        ,start_time\n",
    "        ,month\n",
    "        ,year\n",
    "        ,user_id\n",
    "        ,level\n",
    "        ,song_id\n",
    "        ,artist_id\n",
    "        ,session_id\n",
    "        ,location\n",
    "        ,user_agent\n",
    "      from \n",
    "            (select distinct\n",
    "                    to_timestamp(log.ts/1000) as start_time\n",
    "                   ,month(to_timestamp(log.ts/1000)) as month\n",
    "                   ,year(to_timestamp(log.ts/1000)) as year\n",
    "                   ,log.userid as user_id\n",
    "                   ,log.level as level\n",
    "                   ,song.song_id as song_id\n",
    "                   ,song.artist_id as artist_id\n",
    "                   ,log.sessionid as session_id\n",
    "                   ,log.location as location\n",
    "                   ,log.useragent as user_agent\n",
    "\n",
    "                  FROM        log_data_ftr log \n",
    "                    left join song_data song\n",
    "                         on    log.song = song.title\n",
    "                           and log.length = song.duration\n",
    "                            ) log_join_song\n",
    "                   \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
