# Summary of Project:
- The music App Streaming startup Sparkify has grown their user base and song database even more. Now they want to move their datawarehouse to data lake.

# The information regarding datasides residing in S3:
1. Log Data
• 	These files reside in AWS S3.
•	These are the log files generated by event simulator based on song in song dataset
•	These files are partitioned by year and month
2. Song Data:
• 	These files reside in AWS S3.
•	Contains information about the songs and artist of that song.
•	These files are in JSON format
•	The files are partitioned by first three letters of each song’s track ID

# Creation of IAM Role and Redshift Cluster in Amazon
1. Login to AWS Console
2. From services open Idetity and Access Management Console (IAM).
3. Add user using programatic access and give administrator access to this user using attach policy and then select AdministratorAccess.
4. Download the CSV containing credentials of this IAM role.
5. Add key and Secret in dl.cfg and save the file.

# The steps to perform this analysis:
**Step 1**: Loading dimension table songs
This step has below sub steps:
1. The song file which resides in S3 in JSON format will be read.
2. The temporary view of this dataframe will be created.
3. Specific columns which are required for song table are extracted from this view.
4. Writing data from view to parquet file in S3 will take place.

**Step 2**: Loading Dimension Table Artists
This step has below sub steps:
1. Specific columns which are required for artists table are extracted from this view.
2. Writing data from view to parquet file in S3 will take place.

**Step 3**: Loading Dimension Table Users
1. The log file will be loaded in dataframe.
2. The data is filtered as per NextSong action.
3. Temporary view has been created with this filtered data.
4. Specific columns which are required for users table are extracted from this view.
5. These data will be then written in parquet files in S3.

**Step 4**: Loading Dimension Table Time
1. In log file 'ts' is timestamp column.
2. Two user defined functions are created:
	i. get_timestamp : Conversion of unix timestamp to timestamp
	ii. get_datetime : Conversion of timestamp to datetime.
3. With the help of user defined functions and in build functions time table columns are derived.
4. This data will be then written in parquet files in S3.

**Step 5**: Loading Fact Table Songplays
1. The temporary views log data and song data are joined together to get required columns of songplays table.
2. Data will be then written in parquet files in S3.

# Information regarding files in repository:
1.	etl.py - This file contains all the steps mentioned above.
2.	dl.cfg – This is configuarion file to set IAM access key ID and and its secret access key.
3.  Test_ETL_Steps.ipynb -  This file was initially created to run the step by step process of loading data to one S3 bucket from another S3 bucket.

# How to run python scripts?
  Go to Terminal and then type python etl.py and then press enter. After successful execution of this command, all files from log data and song data are loaded into parquet files in S3.
